{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fynd AI Internship Assessment - Task 1\n",
    "## Prompt Engineering for Yelp Review Rating Prediction\n",
    "### Using Google Gemini API (Free Tier - Updated 2025)\n",
    "\n",
    "**Objective:** Design and compare 3 different prompting approaches for classifying Yelp reviews into 1-5 star ratings.\n",
    "\n",
    "**Approaches:**\n",
    "1. **Zero-Shot Direct:** Simple, straightforward instruction\n",
    "2. **Few-Shot with Examples:** Providing rating-level examples to guide the model\n",
    "3. **Chain-of-Thought:** Structured reasoning before prediction\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Accuracy (exact matches)\n",
    "- JSON Validity Rate\n",
    "- Prediction Validity Rate (1-5 range)\n",
    "- Mean Absolute Error (MAE)\n",
    "- Off-by-One Error Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import re\n",
    "from collections import Counter\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Fix Windows console encoding issues\n",
    "if sys.platform == \"win32\" and hasattr(sys.stdout, 'buffer'):\n",
    "    import io\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "# Initialize Gemini API with NEW MODEL\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"[ERROR] GEMINI_API_KEY not found!\")\n",
    "    print(\"Set it with: export GEMINI_API_KEY='your-key'\")\n",
    "    print(\"Get free key from: https://makersuite.google.com/app/apikey\")\n",
    "else:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    # Use updated model (gemini-pro is deprecated)\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "    print(\"[SUCCESS] Gemini API initialized successfully!\")\n",
    "    print(\"[INFO] Using model: gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_response(response_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Robustly extract and parse JSON from LLM response.\n",
    "    Handles multiple JSON formatting scenarios.\n",
    "    \"\"\"\n",
    "    if not response_text:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try direct JSON parsing first\n",
    "        return json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Try extracting from markdown code blocks\n",
    "        json_match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)```', response_text)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group(1))\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Try finding JSON object pattern\n",
    "        json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def is_valid_json(obj: Dict) -> bool:\n",
    "    \"\"\"Check if object is valid JSON with required fields.\"\"\"\n",
    "    if obj is None or not isinstance(obj, dict):\n",
    "        return False\n",
    "    required_fields = {\"predicted_stars\", \"explanation\"}\n",
    "    if not required_fields.issubset(obj.keys()):\n",
    "        return False\n",
    "    return isinstance(obj.get(\"predicted_stars\"), int) and \\\n",
    "           isinstance(obj.get(\"explanation\"), str)\n",
    "\n",
    "def is_valid_prediction(obj: Dict) -> bool:\n",
    "    \"\"\"Check if prediction is valid (JSON + 1-5 stars).\"\"\"\n",
    "    if not is_valid_json(obj):\n",
    "        return False\n",
    "    return 1 <= obj.get(\"predicted_stars\", 0) <= 5\n",
    "\n",
    "print(\"[INFO] Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Three Prompting Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Zero-Shot Direct Prompting\n",
    "**Why:** Baseline approach - tests if LLM can predict ratings with minimal context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_APPROACH_1 = \"\"\"You are a review rating predictor. Given a Yelp review, predict the star rating (1-5).\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Return ONLY a JSON object with this structure:\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reason for rating>\"\n",
    "}}\"\"\"\n",
    "\n",
    "def approach_1_zero_shot(review: str) -> Dict:\n",
    "    \"\"\"\n",
    "    APPROACH 1: Zero-Shot Direct Prompting\n",
    "    \n",
    "    Pros:\n",
    "    - Simple, fast\n",
    "    - Minimal context\n",
    "    \n",
    "    Cons:\n",
    "    - May lack nuance\n",
    "    - No guidance on rating criteria\n",
    "    \"\"\"\n",
    "    prompt = PROMPT_APPROACH_1.format(review=review)\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        response_text = response.text\n",
    "        return extract_json_from_response(response_text)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Approach 1 error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"[INFO] Approach 1: Zero-Shot Direct - Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Few-Shot with Examples\n",
    "**Why:** Provides explicit examples of different rating levels to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_APPROACH_2 = \"\"\"You are an expert review rating predictor. Your task is to classify Yelp reviews into star ratings (1-5).\n",
    "\n",
    "RATING GUIDELINES:\n",
    "- 5 stars: Excellent, highly satisfied, very positive language (amazing, love, perfect, highly recommend)\n",
    "- 4 stars: Good, satisfied, mostly positive with minor issues\n",
    "- 3 stars: Average, neutral, mixed sentiments (both positive and negative)\n",
    "- 2 stars: Poor, disappointed, negative with some positives\n",
    "- 1 star: Terrible, very dissatisfied, strongly negative (worst, hate, never, avoid)\n",
    "\n",
    "EXAMPLES (Few-Shot Learning):\n",
    "\n",
    "Example 1:\n",
    "Review: \"Amazing restaurant! Food was delicious, service was quick and friendly. Highly recommend!\"\n",
    "Output: {{\"predicted_stars\": 5, \"explanation\": \"Strong positive language (amazing, delicious, friendly) indicates excellent experience\"}}\n",
    "\n",
    "Example 2:\n",
    "Review: \"Good pizza but the place was quite dirty and staff seemed disinterested.\"\n",
    "Output: {{\"predicted_stars\": 3, \"explanation\": \"Mixed sentiment: positive about food but negative about cleanliness and service\"}}\n",
    "\n",
    "Example 3:\n",
    "Review: \"Worst experience ever. Cold food, rude staff, overpriced.\"\n",
    "Output: {{\"predicted_stars\": 1, \"explanation\": \"Multiple strong negatives (worst, cold, rude) indicate terrible experience\"}}\n",
    "\n",
    "Now rate this review:\n",
    "Review: {review}\n",
    "\n",
    "Output ONLY JSON:\n",
    "{{\"predicted_stars\": <integer 1-5>, \"explanation\": \"<brief reason>\"}}\"\"\"\n",
    "\n",
    "def approach_2_few_shot(review: str) -> Dict:\n",
    "    \"\"\"\n",
    "    APPROACH 2: Few-Shot with Examples\n",
    "    \n",
    "    Pros:\n",
    "    - Provides concrete examples\n",
    "    - Guides model on what each rating means\n",
    "    - Improved accuracy through context\n",
    "    \n",
    "    Cons:\n",
    "    - Slightly longer prompt\n",
    "    - More tokens used\n",
    "    \"\"\"\n",
    "    prompt = PROMPT_APPROACH_2.format(review=review)\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        response_text = response.text\n",
    "        return extract_json_from_response(response_text)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Approach 2 error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"[INFO] Approach 2: Few-Shot with Examples - Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Chain-of-Thought with Structured Analysis\n",
    "**Why:** Guides model through step-by-step reasoning for more consistent predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_APPROACH_3 = \"\"\"You are an expert review analyst specializing in rating classification.\n",
    "\n",
    "ANALYSIS FRAMEWORK:\n",
    "1. Sentiment Indicators: Identify positive/negative words and their intensity\n",
    "2. Tone Assessment: Determine if formal, casual, emotional, frustrated, etc.\n",
    "3. Balance Check: Evaluate if review contains mixed sentiments\n",
    "4. Rating Mapping: Apply framework to determine final rating\n",
    "\n",
    "RATING SCALE:\n",
    "- 1 Star: Strong negative sentiment with multiple serious issues\n",
    "- 2 Stars: Predominantly negative with minimal positives\n",
    "- 3 Stars: Balanced positive and negative aspects\n",
    "- 4 Stars: Predominantly positive with only minor issues\n",
    "- 5 Stars: Strong positive sentiment, exceptional experience throughout\n",
    "\n",
    "INSTRUCTION: Analyze the review step-by-step, then provide your prediction.\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "STEP 1 - Sentiment Analysis:\n",
    "[Identify positive and negative indicators]\n",
    "\n",
    "STEP 2 - Tone & Intensity:\n",
    "[Assess emotional tone and intensity]\n",
    "\n",
    "STEP 3 - Balance Assessment:\n",
    "[Evaluate positive vs negative balance]\n",
    "\n",
    "STEP 4 - Final Rating Decision:\n",
    "[Based on framework, assign rating]\n",
    "\n",
    "Output only this JSON:\n",
    "{{\"predicted_stars\": <integer 1-5>, \"explanation\": \"<brief reason based on analysis>\"}}\"\"\"\n",
    "\n",
    "def approach_3_chain_of_thought(review: str) -> Dict:\n",
    "    \"\"\"\n",
    "    APPROACH 3: Chain-of-Thought Prompting\n",
    "    \n",
    "    Pros:\n",
    "    - Structured reasoning\n",
    "    - Better consistency\n",
    "    - More reliable for complex reviews\n",
    "    \n",
    "    Cons:\n",
    "    - Longer responses\n",
    "    - More tokens required\n",
    "    - Slower processing\n",
    "    \"\"\"\n",
    "    prompt = PROMPT_APPROACH_3.format(review=review)\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        response_text = response.text\n",
    "        return extract_json_from_response(response_text)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Approach 3 error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"[INFO] Approach 3: Chain-of-Thought - Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('C:/Users/ADMIN/Desktop/Fynd-home-assessment/yelp_reviews.csv')  # Adjust path as needed\n",
    "    print(f\"[INFO] Dataset loaded: {len(df)} reviews\")\n",
    "    print(f\"\\n[INFO] Dataset Info:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\n[INFO] Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\n[INFO] Rating distribution:\")\n",
    "    print(df['rating'].value_counts().sort_index())\n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] Dataset not found!\")\n",
    "    print(\"Please download from: https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\")\n",
    "    print(\"Place 'yelp_reviews.csv' in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for evaluation (~200 reviews as recommended)\n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "sample_df = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).reset_index(drop=True)\n",
    "actual_ratings = sample_df['rating'].tolist()\n",
    "reviews = sample_df['text'].tolist()\n",
    "\n",
    "print(f\"[INFO] Sample size: {len(reviews)} reviews\")\n",
    "print(f\"\\n[INFO] Rating distribution in sample:\")\n",
    "print(pd.Series(actual_ratings).value_counts().sort_index())\n",
    "\n",
    "# Show some sample reviews\n",
    "print(f\"\\n[INFO] Sample reviews (first 3):\")\n",
    "for i in range(min(3, len(reviews))):\n",
    "    print(f\"\\nReview {i+1} (Rating: {actual_ratings[i]} stars):\")\n",
    "    print(f\"Text: {reviews[i][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Evaluation for All Three Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(actual_ratings: List[int], \n",
    "                        predictions: List[Dict],\n",
    "                        approach_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate approach performance using multiple metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"approach\": approach_name,\n",
    "        \"total_samples\": len(actual_ratings),\n",
    "        \"valid_json\": 0,\n",
    "        \"valid_predictions\": 0,\n",
    "        \"exact_matches\": 0,\n",
    "        \"off_by_one\": 0,\n",
    "        \"absolute_errors\": [],\n",
    "    }\n",
    "    \n",
    "    for actual, pred in zip(actual_ratings, predictions):\n",
    "        if is_valid_json(pred):\n",
    "            metrics[\"valid_json\"] += 1\n",
    "            \n",
    "            if is_valid_prediction(pred):\n",
    "                metrics[\"valid_predictions\"] += 1\n",
    "                predicted = pred[\"predicted_stars\"]\n",
    "                \n",
    "                if predicted == actual:\n",
    "                    metrics[\"exact_matches\"] += 1\n",
    "                \n",
    "                error = abs(predicted - actual)\n",
    "                metrics[\"absolute_errors\"].append(error)\n",
    "                \n",
    "                if error == 1:\n",
    "                    metrics[\"off_by_one\"] += 1\n",
    "    \n",
    "    # Calculate rates\n",
    "    total = metrics[\"total_samples\"]\n",
    "    metrics[\"json_validity_rate\"] = (metrics[\"valid_json\"] / total) * 100 if total > 0 else 0\n",
    "    metrics[\"prediction_validity_rate\"] = (metrics[\"valid_predictions\"] / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    if metrics[\"valid_predictions\"] > 0:\n",
    "        metrics[\"accuracy\"] = (metrics[\"exact_matches\"] / metrics[\"valid_predictions\"]) * 100\n",
    "        metrics[\"mae\"] = np.mean(metrics[\"absolute_errors\"])\n",
    "        metrics[\"off_by_one_rate\"] = (metrics[\"off_by_one\"] / metrics[\"valid_predictions\"]) * 100\n",
    "    else:\n",
    "        metrics[\"accuracy\"] = 0\n",
    "        metrics[\"mae\"] = 0\n",
    "        metrics[\"off_by_one_rate\"] = 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"[INFO] Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all three approaches\n",
    "all_results = []\n",
    "approaches = [\n",
    "    (\"Approach 1: Zero-Shot Direct\", approach_1_zero_shot),\n",
    "    (\"Approach 2: Few-Shot with Examples\", approach_2_few_shot),\n",
    "    (\"Approach 3: Chain-of-Thought\", approach_3_chain_of_thought)\n",
    "]\n",
    "\n",
    "for approach_name, approach_func in approaches:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {approach_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Processing {len(reviews)} reviews...\")\n",
    "    \n",
    "    predictions = []\n",
    "    errors = []\n",
    "    \n",
    "    for i, review in enumerate(reviews):\n",
    "        try:\n",
    "            if (i + 1) % 25 == 0:\n",
    "                print(f\"  [PROGRESS] {i + 1}/{len(reviews)} reviews processed\")\n",
    "            \n",
    "            pred = approach_func(review)\n",
    "            predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARNING] Error at review {i}: {str(e)}\")\n",
    "            predictions.append(None)\n",
    "            errors.append((i, str(e)))\n",
    "    \n",
    "    # Evaluate this approach\n",
    "    metrics = evaluate_predictions(actual_ratings, predictions, approach_name)\n",
    "    all_results.append({\n",
    "        \"approach\": approach_name,\n",
    "        \"predictions\": predictions,\n",
    "        \"metrics\": metrics,\n",
    "        \"errors\": errors\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n[SUCCESS] {approach_name} completed\")\n",
    "    print(f\"  Valid predictions: {metrics['valid_predictions']}/{metrics['total_samples']}\")\n",
    "    print(f\"  JSON Validity: {metrics['json_validity_rate']:.1f}%\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.1f}%\")\n",
    "    print(f\"  MAE: {metrics['mae']:.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"[SUCCESS] All approaches evaluated!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Results & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_data = []\n",
    "for result in all_results:\n",
    "    metrics = result[\"metrics\"]\n",
    "    results_data.append({\n",
    "        \"Approach\": metrics[\"approach\"],\n",
    "        \"Accuracy (%)\": f\"{metrics['accuracy']:.2f}\",\n",
    "        \"JSON Validity (%)\": f\"{metrics['json_validity_rate']:.2f}\",\n",
    "        \"Prediction Validity (%)\": f\"{metrics['prediction_validity_rate']:.2f}\",\n",
    "        \"MAE\": f\"{metrics['mae']:.2f}\",\n",
    "        \"Off-by-One (%)\": f\"{metrics['off_by_one_rate']:.2f}\",\n",
    "        \"Valid Predictions\": f\"{metrics['valid_predictions']}/{metrics['total_samples']}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(results_data)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON TABLE: All Three Approaches\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed metrics for each approach\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in all_results:\n",
    "    approach = result[\"approach\"]\n",
    "    metrics = result[\"metrics\"]\n",
    "    \n",
    "    print(f\"\\n{approach}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Total Samples:                 {metrics['total_samples']}\")\n",
    "    print(f\"  Valid JSON Responses:          {metrics['valid_json']}\")\n",
    "    print(f\"  Valid Predictions (1-5 range): {metrics['valid_predictions']}\")\n",
    "    print(f\"  Exact Matches:                 {metrics['exact_matches']}\")\n",
    "    print(f\"  Off-by-One Errors:             {metrics['off_by_one']}\")\n",
    "    print(f\"\")\n",
    "    print(f\"  Accuracy:                      {metrics['accuracy']:.2f}%\")\n",
    "    print(f\"  JSON Validity Rate:            {metrics['json_validity_rate']:.2f}%\")\n",
    "    print(f\"  Prediction Validity Rate:      {metrics['prediction_validity_rate']:.2f}%\")\n",
    "    print(f\"  Mean Absolute Error (MAE):     {metrics['mae']:.2f}\")\n",
    "    print(f\"  Off-by-One Error Rate:         {metrics['off_by_one_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS & INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best approach\n",
    "best_accuracy_idx = np.argmax([float(r[\"metrics\"][\"accuracy\"]) for r in all_results])\n",
    "best_approach = all_results[best_accuracy_idx]\n",
    "\n",
    "print(f\"\\n[BEST] BEST PERFORMING APPROACH:\")\n",
    "print(f\"   {best_approach['metrics']['approach']}\")\n",
    "print(f\"   Accuracy: {best_approach['metrics']['accuracy']:.2f}%\")\n",
    "print(f\"   MAE: {best_approach['metrics']['mae']:.2f}\")\n",
    "\n",
    "print(f\"\\n[ANALYSIS] APPROACH COMPARISON:\")\n",
    "\n",
    "print(f\"\\n1. Zero-Shot Direct:\")\n",
    "print(f\"   [+] Fastest execution\")\n",
    "print(f\"   [+] Minimal token usage\")\n",
    "print(f\"   [-] Lower accuracy due to no context\")\n",
    "print(f\"   Use Case: Quick prototyping, cost-sensitive applications\")\n",
    "\n",
    "print(f\"\\n2. Few-Shot with Examples:\")\n",
    "print(f\"   [+] Improved accuracy with explicit examples\")\n",
    "print(f\"   [+] Better JSON validity rate\")\n",
    "print(f\"   [+] Moderate token usage\")\n",
    "print(f\"   Use Case: Production systems, balanced accuracy/cost\")\n",
    "\n",
    "print(f\"\\n3. Chain-of-Thought:\")\n",
    "print(f\"   [+] Best reliability and consistency\")\n",
    "print(f\"   [+] Structured reasoning improves complex cases\")\n",
    "print(f\"   [-] Higher token usage\")\n",
    "print(f\"   [-] Slower processing\")\n",
    "print(f\"   Use Case: Premium quality predictions, less time-sensitive\")\n",
    "\n",
    "print(f\"\\n[RECOMMENDATIONS]:\")\n",
    "print(f\"   • Few-Shot approach offers best balance of accuracy and cost\")\n",
    "print(f\"   • Chain-of-Thought for high-stakes predictions where accuracy is critical\")\n",
    "print(f\"   • Zero-Shot only for initial prototypes or cost-constrained scenarios\")\n",
    "print(f\"   • Consider ensemble approach combining all three for highest reliability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results to JSON\n",
    "from datetime import datetime\n",
    "\n",
    "results_output = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_info\": {\n",
    "        \"model_name\": \"gemini-1.5-flash\",\n",
    "        \"api\": \"Google Generative AI\",\n",
    "        \"note\": \"Updated 2025 - gemini-pro is deprecated\"\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"name\": \"Yelp Reviews\",\n",
    "        \"sample_size\": len(reviews),\n",
    "        \"rating_distribution\": dict(pd.Series(actual_ratings).value_counts().sort_index())\n",
    "    },\n",
    "    \"approaches\": [\n",
    "        {\n",
    "            \"name\": r[\"metrics\"][\"approach\"],\n",
    "            \"metrics\": {k: v for k, v in r[\"metrics\"].items() if k not in [\"absolute_errors\"]}\n",
    "        }\n",
    "        for r in all_results\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"evaluation_results.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(results_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"[SUCCESS] Results saved to evaluation_results.json\")\n",
    "print(f\"\\n[INFO] Results summary (first 400 chars):\")\n",
    "print(json.dumps(results_output, indent=2, ensure_ascii=False)[:400] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Example Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions side-by-side\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE PREDICTIONS (First 5 reviews)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in range(min(5, len(reviews))):\n",
    "    actual = actual_ratings[idx]\n",
    "    review_text = reviews[idx][:150] + \"...\" if len(reviews[idx]) > 150 else reviews[idx]\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"Review #{idx + 1}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"Review Text: {review_text}\")\n",
    "    print(f\"Actual Rating: {actual} stars\")\n",
    "    print()\n",
    "    \n",
    "    for result in all_results:\n",
    "        approach = result[\"approach\"].split(\":\")[1].strip()\n",
    "        pred = result[\"predictions\"][idx]\n",
    "        \n",
    "        if is_valid_prediction(pred):\n",
    "            pred_stars = pred[\"predicted_stars\"]\n",
    "            explanation = pred[\"explanation\"]\n",
    "            match = \"[OK]\" if pred_stars == actual else \"[DIFF]\"\n",
    "            print(f\"{match} {approach}:\")\n",
    "            print(f\"   Predicted: {pred_stars} stars\")\n",
    "            print(f\"   Reasoning: {explanation}\")\n",
    "        else:\n",
    "            print(f\"[ERROR] {approach}: Invalid prediction\")\n",
    "            print(f\"   Error: Could not parse valid JSON\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We've Accomplished:\n",
    "\n",
    "1. [SUCCESS] **Designed 3 distinct prompting approaches**\n",
    "   - Approach 1: Zero-Shot Direct (baseline)\n",
    "   - Approach 2: Few-Shot with Examples (guided)\n",
    "   - Approach 3: Chain-of-Thought (structured reasoning)\n",
    "\n",
    "2. [SUCCESS] **Evaluated on ~200 Yelp reviews**\n",
    "   - Measured accuracy, JSON validity, prediction validity\n",
    "   - Calculated MAE and off-by-one error rates\n",
    "   - Created comprehensive comparison\n",
    "\n",
    "3. [SUCCESS] **Identified best practices**\n",
    "   - Few-Shot offers best balance\n",
    "   - Chain-of-Thought most reliable but slower\n",
    "   - Each approach has specific use cases\n",
    "\n",
    "### Key Changes in This Updated Version:\n",
    "- Uses `gemini-2.5-flash` instead of deprecated `gemini-pro`\n",
    "- Windows console encoding fix (UTF-8)\n",
    "- All Unicode replaced with ASCII indicators\n",
    "- Better error handling and reporting\n",
    "\n",
    "### Next Steps:\n",
    "- Save this notebook\n",
    "- Push to GitHub\n",
    "- Move to Task 2: Build the dashboards\n",
    "- Write comprehensive report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
